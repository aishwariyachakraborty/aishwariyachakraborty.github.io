<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-X3P7QNHRFG"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-X3P7QNHRFG');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aishwariya Chakraborty - Blog</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="container">
        <header>
            <nav>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="blog.html">Blog</a></li> <!-- Add this line for the blog link -->
                </ul>
            </nav>
            <hr>
        </header>
        <section class="blog-post">
            <p class="blog-title">NVIDIA Profiling Tools</p>
            <p class="blog-date">Published in January 2026</p>
            <p align="justify">
                The increasing popularity of Nvidia GPUs can be partially attributed to the availability of a stable ecosystem of tools which aid in improving the usability of these accelerators. One such very important set of tools are the profiling tools, which provide fine grained visibility into the hardware as well as software stack built on top of it. This, in turn, helps to find out bottlenecks and pin-point causes of inefficiences in the performance of workloads running on these devices. In this blog, I describe a few of these existing profiling tools along with the functionalities they provide.
                
                Primarily, Nvidia provides three main profiling tools - Nsight Systems, Nsight Compute, and CUPTI. In addition, it also provides a system monitoring tool named Nvidia SMI which is very useful for a systems researcher. Apart from these, there are a few other tools as well, such as Nvidia Data Center GPU Manager (DCGM) and Nsight Graphics, which I will (probably?) cover in a future blog.

                <ul>
                    <li>Nsight Systems

                    </li>
                    <li>Nsight Compute

                    </li>
                    <li>CUPTI - <a href="https://docs.nvidia.com/cupti/main/main.html"></a>CUPTI</a>, the CUDA Profiling Tools Interface, ensures seamless profiling compatibility for CUDA applications across various GPU architectures and CUDA driver versions. It has both C and Python APIs, and can profile code both on CPU and GPU. CUPTI supports tracing, i.e., collection of timestamps and metadata related to various GPU/CUDA events, and profiling, i.e., collection of GPU performance metrics per kernel or set of kernels. PyTorch Profiler interacts with CUPTI under the hood to obtain the various events for trace generation. <br><br>
                        
                    The various functionalities supported by CUPTI are as follows:<br>
                    <ul>
                        <li>It enables low-overhead profiling by providing callback mechanism to notify subscribers of an event of its occurence.</li>
                        <li>It provides the option to profile a specific range within an execution.</li>
                        <li>It supports the capability of sampling the warp program counter and scheduler state to identify reasons for stall.</li>
                        <li>It can collect hardware metrics by sampling of GPU performance monitors and kernel performance metrics at source level by SASS patching.</li>
                        <li>It also provides support for automatically saving and restoring the functional state of the CUDA device.</li>
                    </ul>

                    Few important points to note about CUPTI:<br>



                    </li>
                    <li>Nvidia SMI - 
                        Directly query NVML using c API to get the intermediate memory/compute usage values - https://pypi.org/project/nvidia-ml-py/
                    </li>
                </ul>
            </p>
        </section>
    </div>
</body>
</html>





<!-- 
Nvidia Nsight compute - 
https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html
When profiling an application with NVIDIA Nsight Compute, the user launches the NVIDIA Nsight Compute frontend (either the UI or the CLI) on the host system, 
which in turn starts the actual application as a new process on the target system. While host and target are often the same machine, the target can also be 
a remote system with a potentially different operating system.
The tool inserts its measurement libraries into the application process, which allow the profiler to intercept communication with the CUDA user-mode driver. 
In addition, when a kernel launch is detected, the libraries can collect the requested performance metrics from the GPU. The results are then transferred 
back to the frontend.

Depending on which metrics are to be collected, kernels might need to be replayed one or more times, since not all metrics can be collected in a single pass. 
Kernel replay - save and restore all accessible memory for the kernel for each pass - all metrics pertaining to a specific kernel instance
Application replay - the complete application is run multiple times, so that in each run one of those passes metrics can be collected per kernel. does not 
have memory save-and-restore overhead. - all metrics pertaining to a specific kernel instance
Range replay - captures and replays complete ranges of CUDA API calls and kernel launches within the profiled application. range defined using Profiler 
Start/Stop API or NVTX Ranges - all requested metrics in the range
Application Range Replay - A range of workloads is replayed by re-running the entire application without modifying interactions or saving and restoring memory.

The Memory Chart shows a graphical, logical representation of performance data for memory subunits on and off the GPU. Performance data includes transfer sizes, 
hit rates, number of instructions or requests, etc.

NVIDIA Nsight Compute supports collecting many metrics by sampling the GPU’s performance monitors (PM) periodically at fixed intervals.     
NVIDIA Nsight Compute supports periodic sampling of the warp program counter and warp scheduler state.

Overhead is less for hardware provided metrics. Metrics which don’t have string “sass” in the name fall in this category.
Metrics which have string “sass” in the name - Software instrumented metrics are expensive as CUPTI needs to instrument the kernel to collect these. Further 
these metrics cannot be combined with any other metric in the same pass as otherwise instrumented code will also contribute to the metric value.




Depending on the selected metric, data is collected either through a hardware performance monitor on the GPU, through software patching of the kernel instructions or via a launch or device attribute. -->


